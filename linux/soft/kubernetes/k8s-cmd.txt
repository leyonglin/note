
hostnamectl set-hostname k8s-master01
yum install -y conntrack ntpdate ntp ipvsadm ipset jq iptables curl sysstat libseccomp wget vim net-tools git
设置iptables并且清空规则
systemctl stop firewalld && systemctl disable firewalld yum -y install iptables-services && systemctl start iptables && systemctl enable iptables && iptables -F && service iptables save
关闭selinux和swap
swapoff -a && sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab 
setenforce 0 && sed -i 's/^SELINUX=.*/SELINUX=disabled/' /etc/selinux/config
设置时间
systemctl start chronyd

内核参数
cat > kubernetes.conf <<EOF 
net.bridge.bridge-nf-call-iptables=1 
net.bridge.bridge-nf-call-ip6tables=1 
net.ipv4.ip_forward=1 
net.ipv4.tcp_tw_recycle=0 
vm.swappiness=0          # 禁止使用 swap 空间，只有当系统 OOM 时才允许使用它
vm.overcommit_memory=1    # 不检查物理内存是否够用
vm.panic_on_oom=0		  # 开启 OOM
fs.inotify.max_user_instances=8192 
fs.inotify.max_user_watches=1048576 
fs.file-max=52706963 
fs.nr_open=52706963 
net.ipv6.conf.all.disable_ipv6=1 
net.netfilter.nf_conntrack_max=2310720 
EOF 
cp kubernetes.conf /etc/sysctl.d/kubernetes.conf 
sysctl -p /etc/sysctl.d/kubernetes.conf


设置日志收集服务
mkdir /var/log/journal
mkdir /etc/systemd/journald.conf.d
cat > /etc/systemd/journald.conf.d/99-prophet.conf <<EOF 
[Journal] 
Storage=persistent 
Compress=yes 
SyncIntervalSec=5m 
RateLimitInterval=30s 
RateLimitBurst=1000 
SystemMaxUse=10G
SystemMaxFileSize=200M
MaxRetentionSec=2week
ForwardToSyslog=no 
EOF

升级内核
rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm
yum --enablerepo=elrepo-kernel install -y kernel-lt
grub2-set-default 'CentOS Linux (4.4.189-1.el7.elrepo.x86_64) 7 (Core)'
reboot

开启ipvs(kube-proxy开启ipvs的前置条件)
cat>/etc/sysconfig/modules/ipvs.modules<<EOF
#!/bin/bash
modprobe -- ip_vs
modprobe -- ip_vs_rr
modprobe -- ip_vs_wrr
modprobe -- ip_vs_sh
modprobe -- nf_conntrack_ipv4
EOF
chmod 755 /etc/sysconfig/modules/ipvs.modules&&bash /etc/sysconfig/modules/ipvs.modules&&lsmod|grep -e ip_vs -e nf_conntrack_ipv4

安装docker
yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine
yum-config-manager    --add-repo     https://download.docker.com/linux/centos/docker-ce.repo
yum install -y  device-mapper-persistent-data   lvm2  
yum install docker-ce docker-ce-cli containerd.io

配置docker
mkdir /etc/docker
cat > /etc/docker/daemon.json <<EOF
{
"exec-opts": ["native.cgroupdriver=systemd"], "log-driver": "json-file",
"log-opts": {
"max-size": "100m"
},
"insecure-registries": ["https://hub.atguigu.com"]
} 
EOF
mkdir -p /etc/systemd/system/docker.service.d
systemctl daemon-reload && systemctl restart docker && systemctl enable docker

安装 Kubeadm
cat <<EOF > /etc/yum.repos.d/kubernetes.repo 
[kubernetes]
name=Kubernetes 
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 
enabled=1
gpgcheck=0 
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg 
http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg 
EOF
yum -y install kubeadm-1.15.1 kubectl-1.15.1 kubelet-1.15.1 
systemctl enable kubelet.service

配置初始化文件
kubeadm config print init-defaults > kubeadm-config.yaml    #导出kubeadm初始化配置文件
对配置进行修改
    localAPIEndpoint:
        advertiseAddress: 192.168.66.10     #本机ip
	kubernetesVersion: v1.15.1              #版本
	networking:
        podSubnet: "10.244.0.0/16"          #额外添加
		serviceSubnet: 10.96.0.0/12         #默认
---											#额外添加，指定调度方式未ipvs
apiVersion: kubeproxy.config.k8s.io/v1alpha1 
kind: KubeProxyConfiguration
featureGates:
    SupportIPVSProxyMode: true 
mode: ipvs

初始化
kubeadm init --config=kubeadm-config.yaml --experimental-upload-certs | tee kubeadm-init.log
kubectl get node

安装flannel
wget  https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
kubectl get pod -n kube-system

添加子节点：

kubectl get pod -n kube-system -o wide   #详细查看kube-system名称空间的pod信息
kubectl get pod -n kube-system -w		 #watch监看kube-system名称空间的pod

k8s/
├── init
│   ├── kubeadm-config.yaml
│   └── kubeadm-init.log
└── plugin
    └── flannel
        └── kube-flannel.yml


kubectl explain pod.spec  				#查看字段含义
kubectl  describe pod metadata_name     #查看错误
kubectl log metadata_name -c pod_name   #查看日志

init容器
apiVersion: v1
kind: Pod
metadata:  
  name: myapp-pod  
  labels:    
    app: myapp
spec:  
  containers:  
  - name: myapp-container    
    image: busybox    
    command: ['sh','-c','echo The app is running! && sleep 3600']  
  initContainers:  
  - name: init-myservice    
    image: busybox    
    command: ['sh','-c','until nslookup myservice; do echo waiting for myservice; sleep 2;done;']  
  - name: init-mydb    
    image: busybox    
    command: ['sh','-c','until nslookup mydb; do echo waiting for mydb; sleep 2; done;']
#---
#apiVersion: v1
#kind: Service
#metadata:  
#  name: myservice
#spec:  
#  ports:    
#  - protocol: TCP      
#    port: 80      
#    targetPort: 9376
##---
##kind: Service
##apiVersion: v1
##metadata:  
##  name: mydb
##spec:  
##  ports:    
##  - protocol: TCP      
##    port: 80
##	targetPort: 9377

kubectl create -f init-pod.yml
kubectl delete deployment --all
kubectl delete pod --all
kubectl delete svc nginx-deployment
kubectl delete -f liveness-exec.yml

就绪探针：
apiVersion: v1
kind: Pod
metadata:  
  name: readiness-httpget-pod  
  namespace: default
spec:  
  containers:  
  - name: readiness-httpget-container    
    image: wangyanglinux/myapp:v1    
    imagePullPolicy: IfNotPresent    
    readinessProbe:      
      httpGet:        
        port: 80        
        path: /index1.html      
      initialDelaySeconds: 1      
      periodSeconds: 3

存活探针：
apiVersion: v1
kind: Pod
metadata:  
  name: liveness-exec-pod  
  namespace: default
spec:  
  containers:  
  - name: liveness-exec-container    
    image: hub.atguigu.com/library/busybox    
    imagePullPolicy: IfNotPresent    
    command: ["/bin/sh","-c","touch /tmp/live ; sleep 60; rm -rf /tmp/live; sleep3600"]    
    livenessProbe:      
      exec:        
        command: ["test","-e","/tmp/live"]      
      initialDelaySeconds: 1     
      periodSeconds: 3
	  
	  
apiVersion: v1
kind: Pod
metadata:  
  name: liveness-httpget-pod  
  namespace: default
spec:
  containers:  
  - name: liveness-httpget-container    
    image: hub.atguigu.com/library/myapp:v1    
    imagePullPolicy: IfNotPresent    
    ports:    
    - name: http      
      containerPort: 80    
    livenessProbe:      
      httpGet:        
        port: http        
        path: /index.html      
      initialDelaySeconds: 1      
      periodSeconds: 3      
      timeoutSeconds: 10

apiVersion: v1
kind: Pod
metadata:  
  name: probe-tcp
spec:  
  containers:  
  - name: nginx    
    image: hub.atguigu.com/library/myapp:v1    
    livenessProbe:      
      initialDelaySeconds: 5      
      timeoutSeconds: 1      
      tcpSocket:        
        port: 80

启动和退出
apiVersion: v1
kind: Pod
metadata:  
  name: lifecycle-demo
spec:  
  containers:  
  - name: lifecycle-demo-container    
    image: nginx    
    lifecycle:      
      postStart:        
        exec:          
          command: ["/bin/sh", "-c", "echo Hello from the postStart handler >/usr/share/message"]      
      preStop:        
        exec:
          command: ["/bin/sh", "-c", "echo Hello from the poststop handler >/usr/share/message"]


cat>rs.yml<<EOF
apiVersion: extensions/v1beta1 
kind: ReplicaSet
metadata:
  name: frontend 
spec:
  replicas: 3 
  selector:
    matchLabels:
      tier: frontend 
  template:
    metadata:
      labels:
        tier: frontend 
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3 
	    env:
        - name: GET_HOSTS_FROM 
          value: dns
        ports:
        - containerPort: 80
EOF

kubectl get pod --show-labels
kubectl label pod frontend-mfrkr tier=frontend1 --overwrite=True
kubectl get pod --show-labels

cat>deployment<<eof
apiVersion: extensions/v1beta1 
kind: Deployment
metadata:
  name: nginx-deployment 
spec:
  replicas: 3 
  template:
    metadata: labels:
      app: nginx 
	spec:
      containers:
      - name: nginx
        image: nginx:1.7.9 
	    ports:
        - containerPort: 80
eof

kubectl apply -f deployment.yml  --record
kubectl get deployment
kubectl get rs
kubectl get pod
kubectl scale deployment nginx-deployment --replicas 10    #不改变配置文件
kubectl set image deployment/nginx-deployment nginx=nginx:1.9.1
kubectl rollout undo deployment/nginx-deployment
kubectl rollout undo deployment/nginx-deployment --to-revision=2	## 可以使用 --revision参数指定某个历史版本
kubectl rollout status deployment/nginx-deployment
kubectl rollout history deployment/nginx-deployment
kubectl rollout pause deployment/nginx-deployment	## 暂停 deployment 的更新


服务发现：
apiVersion: apps/v1
kind: Deployment 
metadata:
  name: myapp-deploy 
  namespace: default
spec:
  replicas: 3 
  selector:
    matchLabels: 
	  app: myapp
      release: stabel 
	template:
      metadata: 
	  labels:
        app: myapp 
	    release: stabel 
	    env: test
      spec:
        containers:
        - name: myapp
          image: wangyanglinux/myapp:v2 
		  imagePullPolicy: IfNotPresent 
		  ports:
          - name: http 
            containerPort: 80
---
apiVersion: v1
kind: Service 
metadata:
  name: myapp 
  namespace: default
spec:
  type: ClusterIP 
  selector:
    app: myapp 
    release: stabel
  ports:
  - name: http 
    port: 80
    targetPort: 80


ipvsadm -Ln

cat>headless-svc.yml<<eof
apiVersion: v1
kind: Service 
metadata:
  name: myapp-headless 
  namespace: default
spec:
  selector:
    app: myapp 
  clusterIP: "None" 
  ports:
  - port: 80
    targetPort: 80
eof

kubectl apply -f headless-svc.yml
kubectl get svc
kubectl get pod -n kube-system -o wide
dig -t A myapp-headless.default.svc.cluster.local @10.244.0.8     #没有clsterIP,但是还是可以访问域名


cat>node-svc.yml<<eof
apiVersion: v1
kind: Service 
metadata:
  name: myapp 
  namespace: default
spec:
  type: NodePort 
  selector:
    app: myapp 
    release: stabel
  ports:
  - name: http 
    port: 80
    targetPort: 80
eof

ipvsadm -Ln

loadBalancer 和 nodePort 其实是同一种方式。区别在于 loadBalancer 比 nodePort 多了一步，就是可以调用cloud provider 去创建 LB 来向节点导流


ExternalName：把集群外部的服务引入到集群内部来，在集群内部直接使用
当查询主机 my-service.defalut.svc.cluster.local ( SVC_NAME.NAMESPACE.svc.cluster.local )时，集群的DNS 服务将返回一个值 my.database.example.com  的 CNAME  记录
kind: Service 
apiVersion: v1 
metadata:
  name: my-service
  namespace: default
spec:
  type: ExternalName 
  externalName: hub.atguigu.com



wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/baremetal/service-nodeport.yaml
ingress-nginx: 先创建普通的svc(扁平化网络)，在创建ingress-svc时通过rules字段将普通的svc关联起来
apiVersion: extensions/v1beta1 
kind: Ingress
metadata:
  name: nginx-test
spec:
  rules:
  - host: www1.atguigu.com http:
    paths:
    - path: / 
	  backend:
        serviceName: nginx-svc 
	    servicePort: 80


kubectl get svc
kubectl get svc -n ingress-nginx
kubectl get pod -n ingress-nginx
kubectl exec -it -n ingress-nginx nginx-ingress-controller-799dbf6fbd-x9mtj /bin/bash
grep server_name nginx.conf
kubectl get ingress       #有几个rules字段


configmap：明文
	文件/目录创建：kubectl create configmap game-config --from-file=文件/目录
	字面值创建：kubectl create configmap special-config --from-literal=special.how=very --from- literal=special.type=charm
	查看：kubectl get cm game-config -o yaml
secret：
	Service Account ：用来访问 Kubernetes API，由 Kubernetes 自动创建，并且会自动挂载到 Pod 的/run/secrets/kubernetes.io/serviceaccount目录中（pod数量是动态的，因为使用证书认证，太消耗资源了）
	Opaque ：base64编码格式的Secret，用来存储密码、密钥等（加载到pod中会自动解密）
	kubernetes.io/dockerconﬁgjson ：用来存储私有 docker registry 的认证信息
volume：
1.1 emptydir：pod中容器共享
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: hub.atguigu.com/library/myapp:v1
    name: test-container
    volumeMounts:
    - mountPath: /cache
      name: cache-volume
  - name: livene
    image: busybox
    imagePullPolicy: IfNotPresent
    command: ["/bin/sh","-c","sleep 3600s"]
    volumeMounts:
    - mountPath: /test
      name: cache-volume
  volumes:
  - name: cache-volume
    emptyDir: {}
	
1.2 hostPath：卷将主机节点的文件系统中的文件或目录挂载到集群中
apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: hub.atguigu.com/library/myapp:v1
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      path: /data
      type: Directory

1.3 PV也是集群中的资源，PVC用户存储的请求，pod被删除，pv和pvc也会保持绑定关系，一对一绑定

yum install -y nfs-common nfs-utils rpcbind 
mkdir /nfsdata
chmod 666 /nfsdata
chown nfsnobody /nfsdata 
cat >/etc/exports<<eof
/nfsdata *(rw,no_root_squash,no_all_squash,sync) 
/nfsdata1 *(rw,no_root_squash,no_all_squash,sync) 
/nfsdata2 *(rw,no_root_squash,no_all_squash,sync) 
eof
systemctl start rpcbind
systemctl start nfs
防火墙开放nfs和mountd和111三个端口

#showmount -e 192.168.3.70
#mount -vt nfs 192.168.3.70:/nfsdata /test

创建PV：可以直接被pod使用，但不会这样使用
apiVersion: v1
kind: PersistentVolume 
metadata:
  name: nfspv1 
spec:
  capacity:
    storage: 1Gi 
  accessModes:
  - ReadWriteOnce 
  persistentVolumeReclaimPolicy: Recycle 
  storageClassName: nfs
  nfs:
    path: /data/nfs 
	server: 192.168.3.70
kubectl get pv
创建pvc：
apiVersion: v1 
kind: Service 
metadata:
  name: nginx 
  labels:
    app: nginx 
spec:
  ports:
  - port: 80 
    name: web
  clusterIP: None 
  selector:
    app: nginx
---
apiVersion: apps/v1 
kind: StatefulSet 
metadata:
  name: web 
spec:
  selector:
    matchLabels:
      app: nginx 
  serviceName: "nginx" 
  replicas: 3 
  template:
    metadata:
      labels:
         app: nginx 
    spec:
      containers:
      - name: nginx
        image: hub.atguigu.com/library/myapp:v1 
        ports:
        - containerPort: 80 
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html 
  volumeClaimTemplates:
  - metadata:
      name: www 
    spec:
      accessModes: [ "ReadWriteOnce" ] 
      storageClassName: "nfs" 
      resources:
        requests: 
          storage: 1Gi
删除的时候相反，先删除pod，在删除pvc，在删除pv，最后删物理存储

HELM: 包管理工具
cat>servercount.yml<<eof
apiVersion: v1
kind: ServiceAccount 
metadata:
  name: tiller 
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1beta1 
kind: ClusterRoleBinding
metadata:
   name: tiller 
roleRef:
  apiGroup: rbac.authorization.k8s.io 
  kind: ClusterRole
  name: cluster-admin 
subjects:
- kind: ServiceAccount 
  name: tiller 
  namespace: kube-system
eof

kubectl apply -f servercount.yml
helm init --service-account tiller --skip-refresh    #部署helm
helm help
helm repo update

cat >kubernetes-dashboard.yaml<<eof
image:
  repository: k8s.gcr.io/kubernetes-dashboard-amd64 
  tag: v1.10.1
ingress:
  enabled: true 
  hosts:
  - k8s.frognew.com 
  annotations:
  nginx.ingress.kubernetes.io/ssl-redirect: "true" 
  nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
tls:
- secretName: frognew-com-tls-secret 
  hosts:
  - k8s.frognew.com
rbac:
  clusterAdminRole: true
eof  
  
  
helm install . -n kubernetes-dashboard --namespace kube-system -f kubernetes-dashboard.yaml
可以修改svc服务暴露类为NodePort，就可以在物理机上访问了

访问：https://192.168.3.69:31770/			#通过svc暴露的端口访问
kubectl get secret -n kube-system | grep  kubernetes-dashboard-token
kubectl describe secret kubernetes-dashboard-token-k4pjk -n kube-system     #获取token进行令牌登录

prometheus：kubectl top node
git clone https://github.com/coreos/kube-prometheus.git 
cd kube-prometheus/manifests
	1.grafana-service.yaml
	2.prometheus-service.yaml
	3.alertmanager-service.yaml
		spec:
		type: NodePort       #!!
		ports:
		- name: web 
		  port: 9093
		  targetPort: web 
		  nodePort: 30300    #!!  递增
		  
kubectl create -f manifests/setup
until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ""; done
kubectl create -f manifests/
http://192.168.3.65:30100/    #admin  admin

HPA: Horizontal Pod Autoscaling
kubectl run php-apache --image=gcr.io/google_containers/hpa-example --requests=cpu=200m --expose --port=80
kubectl get pod
kubectl top pod php-apache-69dd84889f-f9jnj									#查看资源
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10   #自动伸展
kubectl get hpa
kubectl run -i --tty load-generator --image=busybox /bin/sh			#压测
	while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done


资源限制 - Pod：requests 要分分配的资源，limits 为最高请求的资源值
resources: 
  limits:
    cpu: "4" 
	memory: 2Gi
  requests: 
    cpu: 250m
    memory: 250Mi
资源限制 - 名称空间
1.apiVersion: v1 
  kind: ResourceQuota
2.apiVersion: v1 
  kind: LimitRange    #default即limit的值   defaultRequest即request的值












鉴权：
useradd devuser

cat >devuser-csr.json<<eof
{
  "CN": "devuser",
  "hosts": [], 
  "key": {
    "algo": "rsa", 
	"size": 2048
  },
"names": [
    {
      "C": "CN",
      "ST": "BeiJing",
      "L": "BeiJing",
      "O": "k8s",
      "OU": "System"
    }
  ]
}
eof

wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 
wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 
chmod +x cfssl*
mv cfssl_linux-amd64 /usr/local/bin/cfssl
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfo

cd /etc/kubernetes/pki
cfssl gencert -ca=ca.crt -ca-key=ca.key -profile=kubernetes /root/k8s/config/permission/cert/devuser/devuser-csr.json | cfssljson -bare devuser
cfssl-certinfo -cert devuser.pem

cd -
export KUBE_APISERVER="https://172.20.0.113:6443"
kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/pki/ca.crt \
--embed-certs=true \
--server=${KUBE_APISERVER} \
--kubeconfig=devuser.kubeconfig

cat devuser.kubeconfig

kubectl config set-credentials devuser \
--client-certificate=/etc/kubernetes/pki/devuser.pem \
--client-key=/etc/kubernetes/pki/devuser-key.pem \
--embed-certs=true \
--kubeconfig=devuser.kubeconfig

cat devuser.kubeconfig

kubectl config set-context kubernetes \
--cluster=kubernetes \
--user=devuser \
--namespace=dev \
--kubeconfig=devuser.kubeconfig

cat devuser.kubeconfig

#kubectl create rolebinding devuser-admin-binding --clusterrole=admin --user=devuser --namespace=dev
cat>pod-reader.yaml<<eof
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: dev 
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
eof

cat>devuser-role-bind.yaml<<eof
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: dev 
subjects:
- kind: User
  name: devuser   
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
eof

kubectl apply -f pod-reader.yaml
kubectl apply -f devuser-role-bind.yaml

cp -f ./devuser.kubeconfig /home/devuser/.kube/config
chown devuser:devuser /home/devuser/.kube/config

su devuser
cd .kube
kubectl config use-context kubernetes --kubeconfig=config 

kubectl get pod --all-namespaces -o wide | grep nginx
  

	










安装harbor
yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine
yum-config-manager    --add-repo     https://download.docker.com/linux/centos/docker-ce.repo
yum install -y  device-mapper-persistent-data   lvm2  
yum install docker-ce docker-ce-cli containerd.io
配置docker
mkdir /etc/docker
cat > /etc/docker/daemon.json <<EOF
{
"exec-opts": ["native.cgroupdriver=systemd"], "log-driver": "json-file",
"log-opts": {
"max-size": "100m"
},
"insecure-registries": ["https://hub.atguigu.com"]
} 
EOF

curl -L https://github.com/docker/compose/releases/download/1.21.0/docker-compose-$(uname -s)-$(uname -m) >  /usr/local/bin/docker-compose
sudo chmod a+x /usr/local/bin/docker-compose
docker-compose -version


Harbor 官官方方地地址址：：https://github.com/vmware/harbor/releases 
配置置harbor.cfg  
	hostname：域名
	ui_url_protocol: 协议
	harbor_admin_password: admin密码

创建证书：
mkdir /data/cert 
openssl genrsa -des3 -out server.key 2048 				#生成私钥
openssl req -new-key server.key -out server.csr 
cp server.key server.key.org 							#备份私钥
openssl rsa -in server.key.org -out server.key    		#取出密码
openssl x509 -req -days 365 -in server.csr-signkey server.key -out server.crt  #生成证书
chmod -R 777 /data/cert

./install.sh

docker login http://hub.atguigu.com
docker tag wangyanglinux/myapp:v1 hub.atguigu.com/library/myapp:v1
docker push hub.atguigu.com/library/myapp:v1
kubectl run nginx-deployment --image=hub.atguigu.com/library/myapp:v1 --port=80 --replicas=1
kubectl get deployment
kubectl get rs
kubectl get pod -o wide
kubectl delete pod nginx-deployment-85756b779-xwdf6
kubectl scale --replicas=3 deployment/nginx-deployment
kubectl get svc
kubectl expose -h
kubectl expose deployment nginx-deployment --port=30000 --target-port=80
ipvsadm -Ln
kubectl get svc
curl ip:30000/hostname.html
kubectl  edit svc nginx-deployment    #修改某些字段直接生效








