日志在/var/log/program/*.log
程序家目录在/usr/local/efk/alone/
es配置config/elasticsearch.yml
	cluster.name: elasticsearch          #集群名，同时也是日志名
	node.name: efk0
	path.data: /var/lib/es/data
	path.logs: /var/log/elasticsearch    #日志目录
	network.host: 0.0.0.0 
	http.cors.enabled: true
	http.cors.allow-origin: "*"

kibana配置config/kibana.yml
	server.host: "0.0.0.0"
	server.name: "kibanaud"
	elasticsearch.hosts: ["http://127.0.0.1:9200"]
	kibana.index: ".kibanaud"

logstash配置
  first-pipeline.conf
	input {
		beats {
			port => "5044"
		}
	}
	filter {
	grok {
		match => { "message" => "%{COMBINEDAPACHELOG}" }
	}
	geoip {
		source => "clientip"
	}
	}
	#在日志中输出
	output {
	stdout { codec => rubydebug }
	}

filebeat配置filebeat.yml
	#=========================== Filebeat inputs =============================
	filebeat.inputs:
	- type: log
	enabled: true
	# 要抓取的文件路径 
	paths:
		- /var/log/nginx/*.log
	# 添加额外的字段
	fields:
		log_source: varlog 
	fields_under_root: true
	# 多行处理
	# 不以"yyyy-MM-dd"这种日期开始的行与前一行合并 
	#multiline.pattern: ^\d{4}-\d{1,2}-\d{1,2}
	#multiline.negate: true
	#multiline.match: after
	
	# 5秒钟扫描一次以检查文件更新
	#scan_frequency: 5s
	# 如果文件1小时都没有更新，则关闭文件句柄
	#close_inactive: 1h  
	# 忽略24小时前的文件
	#ignore_older: 24h
	
	
	#- type: log
	#  enabled: true
	#  paths:
	#    - /usr/local/efk/alone/filebeat/logstash-tutorial.log
	#  fields:
	#    log_source: efk-test
	#  fields_under_root: true
	#multiline.pattern: ^\d{4}-\d{1,2}-\d{1,2}
	#multiline.negate: true
	#multiline.match: after
	#scan_frequency: 5s
	#close_inactive: 1h  
	#ignore_older: 24h
	
	#================================ Outputs =====================================
	
	#-------------------------- Elasticsearch output ------------------------------
	output.elasticsearch:
	# Array of hosts to connect to.
	hosts: ["127.0.0.1:9200"]
	#设置es上的索引名称，这个能更好的辨别是否上传成功，默认是filebeat-%{[beat.version]}-%{+yyyy.MM}",不要认为这是 没有上传成功。
	#  index: "nginx-www-access-%{[beat.version]}-%{+yyyy.MM}"
	#setup.template.name: "nginx"
	#setup.template.pattern: "nginx-*"
	#setup.template.enabled: "true" 
	#setup.template.overwrite: true
	# Optional protocol and basic auth credentials.
	#protocol: "https"
	#username: "elastic"
	#password: "changeme"
	
	#----------------------------- Logstash output --------------------------------
	#output.logstash:
	# The Logstash hosts
	# hosts: ["127.0.0.1:5044"]
	
	# Optional SSL. By default is off.
	# List of root certificates for HTTPS server verifications
	#ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]
	
	# Certificate for SSL client authentication
	#ssl.certificate: "/etc/pki/client/cert.pem"
	
	# Client Certificate Key
	#ssl.key: "/etc/pki/client/cert.key"
	
	
	#启动es
	cd /usr/local/efk/alone/
	/usr/local/efk/alone/es/bin/elasticsearch -d &>>/var/log/elasticsearch/elasticsearch.log &
	
	#启动kibana
	cd /usr/local/efk/alone/
	/usr/local/efk/alone/kibana/bin/kibana  &>>/var/log/kibana/kibana.log &
	
	#启动logstash
		cd /usr/local/efk/alone/logstash
		#bin/logstash -f first-pipeline.conf --config.test_and_exit &>>/var/log/logstash/logstash.log &
		/usr/local/efk/alone/logstash/bin/logstash -f second-pipeline.conf --config.reload.automatic &>>/var/log/logstash/logstash.log &
	
	启动filebeat
		cd /usr/local/efk/alone/filebeat
		rm -f data/registry
			#设置dashboard
			#./filebeat setup --dashboards
		nohup ./filebeat -e -c filebeat.yml -d "publish" &>> /var/log/filebeat/filebeat &


#启动es
cd /usr/local/efk/alone/
/usr/local/efk/alone/es/bin/elasticsearch -d &>>/var/log/elasticsearch/elasticsearch.log &

#启动kibana
cd /usr/local/efk/alone/
nohup /usr/local/efk/alone/kibana/bin/kibana  &>>/var/log/kibana/kibana.log &

#启动logstash
	cd /usr/local/efk/alone/logstash
	#bin/logstash -f first-pipeline.conf --config.test_and_exit &>>/var/log/logstash/logstash.log &
	nohup /usr/local/efk/alone/logstash/bin/logstash -f second-pipeline.conf --config.reload.automatic &>>/var/log/logstash/logstash.log &

启动filebeat
	cd /usr/local/efk/alone/filebeat
	rm -f data/registry
		#设置dashboard
		#./filebeat setup --dashboards
	nohup ./filebeat -e -c filebeat.yml -d "publish" &>> /var/log/filebeat/filebeat &		

##########################################################################################

elasticsearch-head 无法连接elasticsearch的原因和解决
	1、5.x之后不支持直接集成到es中去(plugin目录下) 需要通过(nodejs)额外安装服务
	2、通常会查看请求是否出错，关键是请求没有出错，就是没有返回值：
		1.F12 --> network(出错查看原因) --> console(错误输出)
			请求为option有可能是跨域问题，查看console
	

head插件
1.安装环境支持，需要安装nodejs
	yum install -y nodejs npm
2.下载head插件
	cd /usr/local/
	git clone git://github.com/mobz/elasticsearch-head.git
3.安装依赖包
	cd /usr/local/elasticsearch-head
	npm install 
执行后会生成node_modules文件夹
	#如果遇到异常cnpm不是内部或外部命令 cnpm: command not found，则运行如下脚本,使用淘宝镜像包
	#npm install cnpm -g --registry=https://registry.npm.taobao.org
	#cnpm install

1.修改Gruntfile.js
	cd elasticsearch-head
	vim Gruntfile.js
在该文件中添加如下，务必注意不要漏了添加“，”号，这边的hostname:’*’，表示允许所有IP可以访问,此处也可以修改端口号
	server: {
	options: {
	hostname: '*',  -->97行
	port: 9100,
	base: '.',
	keepalive: true
	}
	}
3.修改elasticsearch-head默认连接地址
	cd _site
	vi app.js
做如下修改，将ip地址修改为对应的服务器的ip地址
	将localhost修改为elasticSearch IP
	this.base_uri = this.config.base_uri || this.prefs.get("app-base_uri") || "http://10.0.0.11:9200";  --> 4374 行
4.修改elasticSearch配置文件并启动ElasticSearch
这边需要修改elasticsearch的配置文件elasticsearch.yml，以允许跨域访问，在文末追加如下代码即可
	http.cors.enabled: true
	http.cors.allow-origin: "*"
5.修改完毕后重新启动ElasticSearch（注意不能使用root权限启动)
6.启动elasticsearch-head
	cd /usr/local/elasticsearch-head
	nohup ./node_modules/grunt/bin/grunt server &>>/var/log/elasticsearch/elasticsearch.log &
访问10.0.0.11:9100就能看到我们集群信息



filebeat模块：只能输出到es上
	配置：
		filebeat.config.modules:
		# Glob pattern for configuration loading
		path: /usr/local/efk/alone/filebeat/modules.d/*.yml
		# Set to true to enable config reloading
		reload.enabled: true
		# Period on which files under path should be checked for changes
		#reload.period: 10s
	查看模块:
		./filebeat modules list
	启用你想运行的模块:
		./filebeat modules enable system nginx mysql
	第4步：设置初始环境：
		./filebeat setup -e
	第5步：运行Filebeat
		./filebeat -e
验证auth：
output.elasticsearch:
  hosts: ["myEShost:9200"]
  username: "filebeat_internal"
  password: "YOUR_PASSWORD" 
setup.kibana:
  host: "mykibanahost:5601"
  username: "my_kibana_user"  
  password: "YOUR_PASSWORD"

filebeat配置文件参数：
filebeat.inputs:
- type: log 　　
  paths:
  - /var/log/nginx/access.log
encoding: 读取的文件的编码
exclude_lines: ['^DBG']  #一组正则表达式，用于匹配你想要排除的行。Filebeat会删除
include_lines:  ['^ERR', '^WARN'] #一组正则表达式，用于匹配你想要包含的行,比exclude_lines优先
harvester_buffer_size #当抓取一个文件时每个harvester使用的buffer的字节数。默认是16384。
max_bytes：单个日志消息允许的最大字节数。超过max_bytes的字节将被丢弃且不会被发送
exclude_files：一组正则表达式，用于匹配你想要忽略的文件。默认没有文件被排除
ignore_older：如果启用，那么Filebeat会忽略在指定的时间跨度之前被修改的文件
close_*：close_*配置项用于在一个确定的条件或者时间点之后关闭harvester
close_inactive：当启用此选项时，如果文件在指定的持续时间内未被获取，则Filebeat将关闭文件句柄
close_renamed：当启用此选项时，Filebeat会在重命名文件时关闭文件处理器
close_removed：当启用此选项时，Filebeat会在删除文件时关闭harvester
close_timeout：当启用此选项是，Filebeat会给每个harvester一个预定义的生命时间
scan_frequency：Filebeat多久检查一次指定路径下的新文件
scan.sort：如果你指定了一个非空的值，那么你可以决定用scan.order的升序或者降序。可能的值是 modtime 和 filename
multiline.pattern：指定用于匹配多行的正则表达式
multiline.negate：定义模式是否被否定。默认false。
multiline.match：指定Filebeat如何把多行合并成一个事件。可选的值是 after 或者 before
multiline.flush_pattern：指定一个正则表达式，多行将从内存刷新到磁盘。
multiline.max_lines：可以合并成一个事件的最大行数。如果一个多行消息包含的行数超过max_lines，则超过的行被丢弃

加载外部配置文件，Filebeat允许将配置分隔为多个较小的配置文件，然后加载外部配置文件。
filebeat.config.inputs:
  enabled: true
  path: configs/*.yml
模块配置
filebeat.config.modules:
  enabled: true
  path: ${path.config}/modules.d/*.yml
 
#output.logstash: 
output.elasticsearch:
  hosts: ["https://localhost:9200"]
  #hosts: ["localhost:5044", "localhost:5045"]
  username: "filebeat_internal"
  password: "YOUR_PASSWORD"
  index: "filebeat-%{[beat.version]}-%{+yyyy.MM.dd}"
ssl.certificate_authorities: ["/etc/pki/root/ca.pem"]
ssl.certificate: "/etc/pki/client/cert.pem"
ssl.key: "/etc/pki/client/cert.key"

output.elasticsearch:
  hosts: ["http://localhost:9200"]
  index: "logs-%{[beat.version]}-%{+yyyy.MM.dd}"
  indices:
    - index: "critical-%{[beat.version]}-%{+yyyy.MM.dd}"
      when.contains:
        message: "CRITICAL"
    - index: "error-%{[beat.version]}-%{+yyyy.MM.dd}"
      when.contains:
        message: "ERR"

setup.template.enabled: "true"
setup.template.name: "filebeat"
setup.template.pattern: "filebeat-*"
#setup.template.fields
setup.template.overwrite
#setup.template.settings._source



es/kibana认证x-pack:
xpack.security.audit.enabled: true      	#启用审计  
xpack.license.self_generated.type: basic	#有trial和basic两种
xpack.security.transport.ssl.enabled: true  #启用认证
	elastic　　超级用户
	Kibana　　用于连接并且和Elasticsearch通信的
	logstash_system     用于在Elasticsearch中存储监控信息
	beats_system    用于在Elasticsearch中存储监控信息
	


	
###########################################################################	
	

	
	
	
查看集群健康状态：curl  "localhost:9200/_cat/health?v"
	Green ： everything is good（一切都很好）（所有功能正常）
	Yellow ： 所有数据都是可用的，但有些副本还没有分配（所有功能正常）
	Red ： 有些数据不可用（部分功能正常）
查看索引：curl "localhost:9200/_cat/indices?v"	
创建索引：curl -X PUT "localhost:9200/customer?pretty"       
	#pretty的意思是以JSON格式返回响应，Elasticsearch默认情况下为这个索引创建了一个副本。由于目前我们只有一个节点在运行，所以直到稍后另一个节点加入集群时，才会分配一个副本(对于高可用性)。一旦该副本分配到第二个节点上，该索引的健康状态将变为green。
删除索引：curl -X DELETE "localhost:9200/customer?pretty"	
创建文档：curl -X PUT "localhost:9200/customer/_doc/1?pretty" -H 'Content-Type: application/json' -d'{"name": "John Doe"}'      #如果没有索引会自动创建索引
查看文档：curl  "localhost:9200/customer/_doc/1?pretty"
删除文档：curl -X DELETE "localhost:9200/customer/_doc/2?pretty"

Elasticsearch还可以使用_bulk API批量同时执行增删查改操作
	curl -X POST "localhost:9200/customer/_doc/_bulk?pretty" -H 'Content-Type: application/json' -d'
	{"update":{"_id":"1"}}
	{"doc": { "name": "John Doe becomes Jane Doe" } }
	{"delete":{"_id":"2"}}
	'
导入文档：curl -H "Content-Type: application/json" -XPOST "localhost:9200/bank/_doc/_bulk?pretty&refresh" --data-binary "@accounts.json"	

The Search API
	运行搜索有两种基本方法：一种是通过REST请求URI发送检索参数，另一种是通过REST请求体发送检索参数。
	一种是把检索参数放在URL后面，另一种是放在请求体里面。相当于HTTP的GET和POST请求
		把检索参数放在URL后面(输出格式直观，但不好拓展)
		curl -X GET "localhost:9200/bank/_search?q=*&sort=account_number:asc&pretty"
			took ： Elasticsearch执行搜索的时间（以毫秒为单位）
			timed_out ： 告诉我们检索是否超时
			_shards ： 告诉我们检索了多少分片，以及成功/失败的分片数各是多少
			hits ： 检索的结果
			hits.total ： 符合检索条件的文档总数
			hits.hits ： 实际的检索结果数组（默认为前10个文档）
			hits.sort ： 排序的key（如果按分值排序的话则不显示）
			hits._score 和 max_score 现在我们先忽略这些字段
		把检索参数放在放在请求体里面(输出格式不直观，但好拓展)
		curl -X GET "localhost:9200/bank/_search" -H 'Content-Type: application/json' -d'
		{
		"query": { "match_all": {} },
		"sort": [
			{ "account_number": "asc" }
		]}'

查询语言	
Elasticsearch提供了一种JSON风格的语言，您可以使用这种语言执行查询。这被成为查询DSL	
bool查询允许我们使用布尔逻辑将较小的查询组合成较大的查询。
range查询，它允许我们通过一系列值筛选文档。这通常用于数字或日期查询
分数是一个数值，它是文档与我们指定的搜索查询匹配程度的相对度量/相似度
聚集：相当于SQL中的聚集函数，比如分组、求和、求平均数之类的
把检索参数放在放在请求体里面	

# 查看所有用户
curl -X GET -u elastic "localhost:9200/_xpack/security/user?pretty"
# 查看指定用户
curl -X GET -u elastic "localhost:9200/_xpack/security/user/jacknich?pretty"
创建用户
curl -X POST -u elastic "localhost:9200/_xpack/security/user/jacknich" -H 'Content-Type: application/json' -d'
{
  "password" : "j@rV1s",					#必须的
  "roles" : [ "admin", "other_role1" ],		#必须的
  "full_name" : "Jack Nicholson",
  "email" : "jacknich@example.com",
  "metadata" : {
    "intelligence" : 7
  }
}
'
修改密码
curl -X POST "localhost:9200/_xpack/security/user/jacknich/_password" -H 'Content-Type: application/json' -d'
{
  "password" : "s3cr3t"
}
'
禁用/启用/删除用户
curl -X PUT "localhost:9200/_xpack/security/user/jacknich/_disable"
curl -X PUT "localhost:9200/_xpack/security/user/jacknich/_enable"
curl -X DELETE "localhost:9200/_xpack/security/user/jacknich"	
	
	
	
	
###############################################################################################
logstash:
	No configuration found in the configured sources  #程序家目录权限不对	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	